
import cv2
import dlib
import numpy as np
import os
import urllib.request
import bz2
from math import hypot
from matplotlib import pyplot as plt
from scipy.spatial import distance as dist
from collections import deque
import time
import random
import pygame
from pygame import mixer
from gtts import gTTS
import tempfile
import mediapipe as mp
from tensorflow.keras.models import load_model
from PIL import Image, ImageDraw, ImageFont

# Initialize pygame for sound
pygame.init()
mixer.init()

# Constants
EMOTION_COLORS = {
    "HAPPY": (0, 255, 0),
    "SAD": (255, 0, 0),
    "ANGRY": (0, 0, 255),
    "SURPRISED": (0, 255, 255),
    "NEUTRAL": (200, 200, 200),
    "DISGUSTED": (128, 0, 128),
    "FEARFUL": (255, 165, 0),
    "CONFUSED": (255, 255, 0),
    "EXCITED": (255, 0, 255)
}

EMOTION_EMOJIS = {
    "HAPPY": "😊",
    "SAD": "😢",
    "ANGRY": "😠",
    "SURPRISED": "😲",
    "NEUTRAL": "😐",
    "DISGUSTED": "🤢",
    "FEARFUL": "😨",
    "CONFUSED": "😕",
    "EXCITED": "🤩"
}

def eye_aspect_ratio(eye):
    # Convert landmarks to (x,y) coordinates first
    eye = [(point.x, point.y) for point in eye]
    
    # Compute the Euclidean distances between the two sets of vertical eye landmarks
    A = dist.euclidean(eye[1], eye[5])
    B = dist.euclidean(eye[2], eye[4])
    
    # Compute the Euclidean distance between the horizontal eye landmarks
    C = dist.euclidean(eye[0], eye[3])
    
    # Compute the eye aspect ratio
    ear = (A + B) / (2.0 * C)
    return ear

def detect_emotion(landmarks, frame_face=None):
    # Calculate eye distance first
    left_eye = (landmarks.part(36).x, landmarks.part(36).y)
    right_eye = (landmarks.part(45).x, landmarks.part(45).y)
    eye_distance = hypot(left_eye[0] - right_eye[0], left_eye[1] - right_eye[1])
    
    # Calculate mouth openness
    mouth_width = hypot(landmarks.part(54).x - landmarks.part(48).x,
                       landmarks.part(54).y - landmarks.part(48).y)
    mouth_height = hypot(landmarks.part(62).x - landmarks.part(66).x,
                        landmarks.part(62).y - landmarks.part(66).y)
    mouth_ratio = mouth_height / (mouth_width + 1e-6)
    
    # Calculate eye openness
    left_eye_aspect_ratio = eye_aspect_ratio([landmarks.part(i) for i in range(36, 42)])
    right_eye_aspect_ratio = eye_aspect_ratio([landmarks.part(i) for i in range(42, 48)])
    avg_eye_ratio = (left_eye_aspect_ratio + right_eye_aspect_ratio) / 2
    
    # Calculate eyebrow position
    left_eyebrow = np.mean([landmarks.part(i).y for i in range(17, 22)])
    right_eyebrow = np.mean([landmarks.part(i).y for i in range(22, 27)])
    eyebrow_pos = (left_eyebrow + right_eyebrow) / 2
    nose_top = landmarks.part(27).y
    
    # Nose wrinkling (for disgust)
    nose_width = hypot(landmarks.part(31).x - landmarks.part(35).x,
                      landmarks.part(31).y - landmarks.part(35).y)
    
    # Emotion detection logic
    if mouth_ratio > 0.3 and avg_eye_ratio > 0.25:
        return "SURPRISED", EMOTION_COLORS["SURPRISED"]
    elif mouth_ratio > 0.2:
        if eyebrow_pos < nose_top - 5:
            if random.random() < 0.2:  # 20% chance to be excited when happy
                return "EXCITED", EMOTION_COLORS["EXCITED"]
            return "HAPPY", EMOTION_COLORS["HAPPY"]
        else:
            return "NEUTRAL", EMOTION_COLORS["NEUTRAL"]
    elif avg_eye_ratio < 0.15:
        if nose_width > eye_distance * 0.3:
            return "DISGUSTED", EMOTION_COLORS["DISGUSTED"]
        return "SAD", EMOTION_COLORS["SAD"]
    elif eyebrow_pos > nose_top + 10:
        return "ANGRY", EMOTION_COLORS["ANGRY"]
    elif avg_eye_ratio > 0.3 and mouth_ratio < 0.1:
        return "FEARFUL", EMOTION_COLORS["FEARFUL"]
    elif frame_face is not None:  # Use CNN model as fallback
        return detect_emotion_cnn(frame_face)
    else:
        if random.random() < 0.1:  # 10% chance to be confused when neutral
            return "CONFUSED", EMOTION_COLORS["CONFUSED"]
        return "NEUTRAL", EMOTION_COLORS["NEUTRAL"]

def detect_emotion_cnn(face_roi):
    try:
        # Preprocess the face for the model
        gray = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)
        gray = cv2.resize(gray, (64, 64))
        gray = gray.astype('float32') / 255.0
        gray = np.expand_dims(gray, -1)
        gray = np.expand_dims(gray, 0)
        
        # Predict emotion
        predictions = emotion_model.predict(gray)[0]
        emotion_index = np.argmax(predictions)
        emotion = emotion_labels[emotion_index].upper()
        
        # Map to our emotion set
        if emotion == "SURPRISE":
            emotion = "SURPRISED"
        elif emotion == "HAPPY":
            if random.random() < 0.2:
                emotion = "EXCITED"
        elif emotion == "NEUTRAL" and random.random() < 0.1:
            emotion = "CONFUSED"
            
        return emotion, EMOTION_COLORS.get(emotion, EMOTION_COLORS["NEUTRAL"])
    except:
        return "NEUTRAL", EMOTION_COLORS["NEUTRAL"]

class EmotionVisualizer:
    def __init__(self):
        self.emotion_history = deque(maxlen=100)
        self.fig = plt.figure(figsize=(18, 10))
        self.ax1 = self.fig.add_subplot(241)
        self.ax2 = self.fig.add_subplot(242)
        self.ax3 = self.fig.add_subplot(243)
        self.ax4 = self.fig.add_subplot(244)
        self.ax5 = self.fig.add_subplot(212)
        plt.ion()
        
        self.current_emotion = "NEUTRAL"
        self.last_emotion_change = time.time()
        self.emotion_duration = 0
        self.last_sound_time = 0
        
        # Enhanced emotion filters with animations
        self.emotion_filters = {
            "HAPPY": self.apply_happy_filter,
            "SAD": self.apply_sad_filter,
            "ANGRY": self.apply_angry_filter,
            "SURPRISED": self.apply_surprised_filter,
            "DISGUSTED": self.apply_disgusted_filter,
            "FEARFUL": self.apply_fearful_filter,
            "NEUTRAL": lambda x: x,
            "CONFUSED": self.apply_confused_filter,
            "EXCITED": self.apply_excited_filter
        }
        
        # Emotion sound effects
        self.sound_effects = {
            "HAPPY": "happy_sound.mp3",
            "SAD": "sad_sound.mp3",
            "ANGRY": "angry_sound.mp3",
            "SURPRISED": "surprised_sound.mp3"
        }
        
        # Emotion responses
        self.emotion_responses = {
            "HAPPY": ["You look happy! 😊", "What's making you smile today?", "Your joy is contagious!"],
            "SAD": ["I see you're feeling down...", "Would you like to talk about it?", "It's okay to feel sad sometimes"],
            "ANGRY": ["Take a deep breath...", "Try to stay calm", "Anger is temporary"],
            "SURPRISED": ["Wow! What surprised you?", "That's unexpected!", "Surprise!"],
            "EXCITED": ["You're excited! 😃", "What's got you so pumped?", "Your energy is amazing!"],
            "CONFUSED": ["You seem puzzled", "What's on your mind?", "Confusion is the first step to learning"]
        }
        
        # Load default sound effects
        self.load_default_sounds()
    
    def load_default_sounds(self):
        # Create temporary sound files
        try:
            # Happy sound
            tts = gTTS(text="Happy!", lang='en')
            happy_path = os.path.join(tempfile.gettempdir(), "happy_sound.mp3")
            tts.save(happy_path)
            
            # Sad sound
            tts = gTTS(text="Oh no...", lang='en')
            sad_path = os.path.join(tempfile.gettempdir(), "sad_sound.mp3")
            tts.save(sad_path)
            
            # Angry sound
            tts = gTTS(text="Calm down!", lang='en')
            angry_path = os.path.join(tempfile.gettempdir(), "angry_sound.mp3")
            tts.save(angry_path)
            
            # Surprised sound
            tts = gTTS(text="Wow!", lang='en')
            surprised_path = os.path.join(tempfile.gettempdir(), "surprised_sound.mp3")
            tts.save(surprised_path)
            
            self.sound_effects = {
                "HAPPY": happy_path,
                "SAD": sad_path,
                "ANGRY": angry_path,
                "SURPRISED": surprised_path
            }
        except:
            print("Couldn't generate sound effects, continuing without audio")
            self.sound_effects = {}
    
    def apply_happy_filter(self, frame):
        # Sunshine effect with animated rays
        overlay = frame.copy()
        h, w = frame.shape[:2]
        
        # Increase saturation and brightness
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        hsv[:,:,1] = hsv[:,:,1] * 1.3
        hsv[:,:,2] = np.minimum(hsv[:,:,2] * 1.2, 255)
        frame = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
        
        # Add animated sun rays
        ray_length = int(time.time() * 5) % 30 + 20
        center = (w//2, h//4)
        for i in range(12):
            angle = i * 30 + (time.time() * 20) % 30
            end_point = (
                int(center[0] + ray_length * np.cos(np.radians(angle))),
                int(center[1] + ray_length * np.sin(np.radians(angle))))
            cv2.line(overlay, center, end_point, (0, 255, 255), 2)
        
        # Combine with frame
        frame = cv2.addWeighted(overlay, 0.3, frame, 0.7, 0)
        
        # Add floating emojis occasionally
        if random.random() < 0.02:
            emoji = EMOTION_EMOJIS["HAPPY"]
            self.add_floating_emoji(frame, emoji)
            
        return frame
    
    def apply_sad_filter(self, frame):
        # Blue tint with rain effect
        blue_tint = np.full(frame.shape, (255, 100, 0), dtype=np.uint8)
        frame = cv2.addWeighted(frame, 0.7, blue_tint, 0.3, 0)
        
        # Add animated rain
        h, w = frame.shape[:2]
        overlay = np.zeros_like(frame)
        rain_positions = getattr(self, 'rain_positions', [])
        
        if not rain_positions or random.random() < 0.1:
            rain_positions = [(random.randint(0, w), random.randint(-50, 0)) 
                            for _ in range(50)]
        
        for i, (x, y) in enumerate(rain_positions):
            y = (y + 5) % (h + 50)
            cv2.line(overlay, (x, y), (x, y+10), (200, 200, 255), 1)
            rain_positions[i] = (x, y)
        
        self.rain_positions = rain_positions
        frame = cv2.addWeighted(frame, 0.9, overlay, 0.3, 0)
        
        return frame
    
    def apply_angry_filter(self, frame):
        # Red vignette with fire effect
        rows, cols = frame.shape[:2]
        mask = np.zeros((rows, cols), dtype=np.uint8)
        cv2.ellipse(mask, (cols//2, rows//2), (cols//2, rows//2), 0, 0, 360, 255, -1)
        mask = cv2.merge([mask, mask, mask])
        
        # Create fire-like overlay
        fire_overlay = np.zeros_like(frame)
        for i in range(10):
            x = random.randint(0, cols)
            y = random.randint(rows-50, rows)
            size = random.randint(20, 50)
            color = (0, 0, random.randint(200, 255))
            cv2.circle(fire_overlay, (x, y), size, color, -1)
            cv2.circle(fire_overlay, (x, y-size//2), size//2, (0, 0, 255), -1)
        
        frame = cv2.addWeighted(frame, 0.7, cv2.bitwise_and(fire_overlay, mask), 0.5, 0)
        
        # Add occasional ember particles
        if random.random() < 0.3:
            for _ in range(random.randint(5, 15)):
                x = random.randint(0, cols)
                y = random.randint(0, rows)
                cv2.circle(frame, (x, y), 2, (0, 0, 255), -1)
        
        return frame
    
    def apply_surprised_filter(self, frame):
        # Flash effect with circular waves
        flash_strength = 0.2 + 0.1 * np.sin(time.time() * 5)
        frame = cv2.addWeighted(frame, 1 - flash_strength, 
                              np.full(frame.shape, 255, dtype=np.uint8), 
                              flash_strength, 0)
        
        # Add circular shockwave
        h, w = frame.shape[:2]
        center = (w//2, h//2)
        radius = int((time.time() * 10) % (min(w, h)//2))
        cv2.circle(frame, center, radius, (255, 255, 255), 2)
        
        return frame
    
    def apply_disgusted_filter(self, frame):
        # Green tint with swirling effect
        h, w = frame.shape[:2]
        y, x = np.indices((h, w))
        offset_x = int(10 * np.sin(time.time() * 2))
        offset_y = int(10 * np.cos(time.time() * 2))
        
        # Create displacement map
        map_x = x + offset_x * np.sin(y / 50 + time.time())
        map_y = y + offset_y * np.cos(x / 50 + time.time())
        
        # Apply remapping for swirling effect
        frame = cv2.remap(frame, map_x.astype(np.float32), map_y.astype(np.float32), 
                         cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)
        
        # Add green tint
        green_tint = np.full(frame.shape, (0, 100, 0), dtype=np.uint8)
        frame = cv2.addWeighted(frame, 0.8, green_tint, 0.2, 0)
        
        return frame
    
    def apply_fearful_filter(self, frame):
        # Dark vignette with shaking effect
        rows, cols = frame.shape[:2]
        
        # Shake effect
        dx = int(5 * np.sin(time.time() * 10))
        dy = int(5 * np.cos(time.time() * 10))
        M = np.float32([[1, 0, dx], [0, 1, dy]])
        frame = cv2.warpAffine(frame, M, (cols, rows))
        
        # Dark vignette
        mask = np.zeros((rows, cols), dtype=np.uint8)
        cv2.circle(mask, (cols//2, rows//2), min(rows, cols)//2, 255, -1)
        mask = cv2.merge([mask, mask, mask])
        frame = cv2.addWeighted(frame, 0.6, cv2.bitwise_and(np.zeros_like(frame), mask), 0.4, 0)
        
        # Add random dark spots
        if random.random() < 0.1:
            for _ in range(random.randint(3, 8)):
                x = random.randint(0, cols)
                y = random.randint(0, rows)
                size = random.randint(10, 30)
                cv2.circle(frame, (x, y), size, (0, 0, 0), -1)
        
        return frame
    
    def apply_confused_filter(self, frame):
        # Wobble effect with question marks
        h, w = frame.shape[:2]
        
        # Create wobble effect
        map_x = np.zeros((h, w), np.float32)
        map_y = np.zeros((h, w), np.float32)
        
        for y in range(h):
            for x in range(w):
                offset = 5 * np.sin(x / 30 + time.time())
                map_x[y, x] = x + offset
                map_y[y, x] = y + offset * 0.5
        
        frame = cv2.remap(frame, map_x, map_y, cv2.INTER_LINEAR)
        
        # Add floating question marks occasionally
        if random.random() < 0.05:
            emoji = EMOTION_EMOJIS["CONFUSED"]
            self.add_floating_emoji(frame, emoji)
        
        return frame
    
    def apply_excited_filter(self, frame):
        # Sparkle effect with color shifts
        h, w = frame.shape[:2]
        
        # Color cycling
        hue_shift = int((time.time() * 50) % 180)
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        hsv[:,:,0] = (hsv[:,:,0] + hue_shift) % 180
        frame = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
        
        # Add sparkles
        if random.random() < 0.3:
            for _ in range(random.randint(5, 20)):
                x = random.randint(0, w)
                y = random.randint(0, h)
                color = (random.randint(200, 255), random.randint(200, 255), random.randint(200, 255))
                size = random.randint(1, 3)
                cv2.circle(frame, (x, y), size, color, -1)
                if random.random() < 0.5:
                    cv2.line(frame, (x-size, y), (x+size, y), color, 1)
                    cv2.line(frame, (x, y-size), (x, y+size), color, 1)
        
        # Add floating emojis occasionally
        if random.random() < 0.03:
            emoji = EMOTION_EMOJIS["EXCITED"]
            self.add_floating_emoji(frame, emoji)
        
        return frame
    
    def add_floating_emoji(self, frame, emoji, size=30):
        try:
            # Convert frame to PIL image
            img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            draw = ImageDraw.Draw(img_pil)
            
            # Use a system font that supports emoji
            font = ImageFont.truetype("arial.ttf", size)
            
            # Random position
            x = random.randint(50, frame.shape[1] - 50)
            y = random.randint(50, frame.shape[0] - 50)
            
            # Draw emoji
            draw.text((x, y), emoji, font=font, embedded_color=True)
            
            # Convert back to OpenCV format
            frame[:] = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
        except:
            pass  # Skip if emoji can't be rendered
    
    def play_emotion_sound(self, emotion):
        if time.time() - self.last_sound_time < 5:  # Don't play sounds too frequently
            return
            
        if emotion in self.sound_effects:
            try:
                sound_path = self.sound_effects[emotion]
                mixer.music.load(sound_path)
                mixer.music.play()
                self.last_sound_time = time.time()
            except:
                pass
    
    def speak_emotion_response(self, emotion):
        if time.time() - self.last_sound_time < 10:  # Don't speak too frequently
            return
            
        if emotion in self.emotion_responses:
            response = random.choice(self.emotion_responses[emotion])
            try:
                tts = gTTS(text=response, lang='en')
                with tempfile.NamedTemporaryFile(delete=True) as fp:
                    tts.save(fp.name + '.mp3')
                    mixer.music.load(fp.name + '.mp3')
                    mixer.music.play()
                    self.last_sound_time = time.time()
            except:
                pass
    
    def update(self, frame, emotion):
        # Track emotion duration
        if emotion != self.current_emotion:
            self.current_emotion = emotion
            self.last_emotion_change = time.time()
            self.emotion_duration = 0
            
            # Play sound and speak for new emotions
            self.play_emotion_sound(emotion)
            if random.random() < 0.3:  # 30% chance to speak
                self.speak_emotion_response(emotion)
        else:
            self.emotion_duration = time.time() - self.last_emotion_change
        
        # Apply emotion-specific filter
        filtered_frame = self.emotion_filters.get(self.current_emotion, lambda x: x)(frame.copy())
        
        # Update camera feed
        self.ax1.clear()
        self.ax1.imshow(cv2.cvtColor(filtered_frame, cv2.COLOR_BGR2RGB))
        self.ax1.set_title(f"Current Emotion: {emotion} {EMOTION_EMOJIS.get(emotion, '')}\nDuration: {self.emotion_duration:.1f}s")
        self.ax1.axis('off')
        
        # Update emotion history
        self.emotion_history.append(emotion)
        
        # Update emotion distribution pie chart
        self.ax2.clear()
        emotions = list(EMOTION_COLORS.keys())
        counts = [self.emotion_history.count(e) for e in emotions]
        self.ax2.pie([max(c, 0.1) for c in counts], labels=emotions, autopct='%1.1f%%', 
                    colors=[tuple(np.array(EMOTION_COLORS[e])/255) for e in emotions])
        self.ax2.set_title("Emotion Distribution")
        
        # Update emotion duration bar chart
        self.ax3.clear()
        self.ax3.bar(emotions, counts, color=[tuple(np.array(EMOTION_COLORS[e])/255) for e in emotions])
        self.ax3.set_title("Emotion Frequency")
        plt.setp(self.ax3.get_xticklabels(), rotation=45)
        
        # Update emotion timeline
        self.ax4.clear()
        numeric_history = [emotions.index(e) if e in emotions else len(emotions)-1 
                          for e in self.emotion_history]
        self.ax4.plot(numeric_history, marker='o', linestyle='-', color='purple')
        self.ax4.set_yticks(range(len(emotions)))
        self.ax4.set_yticklabels(emotions)
        self.ax4.set_title("Emotion Timeline")
        self.ax4.grid(True)
        
        # Update emotion intensity graph
        self.ax5.clear()
        window_size = 5
        smoothed_history = []
        for i in range(len(numeric_history)):
            start = max(0, i - window_size)
            end = min(len(numeric_history), i + window_size + 1)
            smoothed_history.append(np.mean(numeric_history[start:end]))
        
        self.ax5.plot(smoothed_history, color='blue', linewidth=2)
        self.ax5.set_title("Emotion Intensity Over Time")
        self.ax5.set_xlabel("Time (frames)")
        self.ax5.set_ylabel("Emotion Intensity")
        self.ax5.grid(True)
        
        plt.tight_layout()
        self.fig.canvas.draw()
        self.fig.canvas.flush_events()
    
    def show_final(self):
        plt.ioff()
        plt.figure(figsize=(15, 8))
        
        # Convert colors from 0-255 to 0-1 range for matplotlib
        emotions = list(EMOTION_COLORS.keys())
        counts = [self.emotion_history.count(e) for e in emotions]
        colors = [tuple(np.array(EMOTION_COLORS[e])/255) for e in emotions]
        
        # Emotion distribution
        plt.subplot(121)
        plt.pie([max(c, 0.1) for c in counts], labels=emotions, autopct='%1.1f%%',
                colors=colors)
        plt.title("Emotion Distribution")
        
        # Emotion timeline
        plt.subplot(122)
        numeric_history = [emotions.index(e) if e in emotions else len(emotions)-1 
                          for e in self.emotion_history]
        plt.plot(numeric_history, color='purple')
        plt.yticks(range(len(emotions)), emotions)
        plt.title("Your Emotional Journey")
        plt.xlabel("Time (frames)")
        plt.grid(True)
        
        plt.tight_layout()
        plt.show()

def main():
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("Error: Could not open camera.")
        return
    
    # Set camera resolution
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
    
    visualizer = EmotionVisualizer()
    print("Advanced Emotion Detection started. Press 'q' to quit.")
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("Error: Couldn't read frame.")
                break
            
            frame = cv2.flip(frame, 1)
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = detector(gray)
            
            if len(faces) > 0:
                # Get face bounding box
                face = faces[0]
                x, y, w, h = face.left(), face.top(), face.width(), face.height()
                
                # Extract face ROI for CNN model
                face_roi = frame[y:y+h, x:x+w]
                if face_roi.size == 0:
                    face_roi = None
                
                # Get landmarks
                landmarks = predictor(gray, face)
                emotion, color = detect_emotion(landmarks, face_roi)
                
                # Draw facial landmarks
                for n in range(0, 68):
                    x = landmarks.part(n).x
                    y = landmarks.part(n).y
                    cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)
                
                # Draw face bounding box with emotion color
                cv2.rectangle(frame, (face.left(), face.top()), 
                             (face.right(), face.bottom()), color, 2)
                
                # Display emotion with animated text
                text = f"{emotion} {EMOTION_EMOJIS.get(emotion, '')}"
                text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)[0]
                text_x = (frame.shape[1] - text_size[0]) // 2
                
                # Animated background for text
                text_bg_height = text_size[1] + 20
                text_bg_width = text_size[0] + 40
                cv2.rectangle(frame, 
                             (text_x - 20, 30 - 10),
                             (text_x + text_size[0] + 20, 30 + text_size[1] + 10),
                             color, -1)
                
                # Pulsing text effect
                text_scale = 1.0 + 0.1 * np.sin(time.time() * 5)
                cv2.putText(frame, text, (text_x, 30 + text_size[1]), 
                          cv2.FONT_HERSHEY_SIMPLEX, text_scale, 
                          (255, 255, 255), 2)
                
                # Add emotion-specific overlay
                visualizer.update(frame, emotion)
            
            # Display raw frame
            cv2.imshow('Advanced Emotion Detection', frame)
            
            # Exit on 'q' key
            if cv2.waitKey(1) & 0xFF == ord('q'):
                print("Quitting...")
                break
                
    finally:
        cap.release()
        cv2.destroyAllWindows()
        visualizer.show_final()
        print("Session ended.")

if __name__ == "__main__":
    main().

