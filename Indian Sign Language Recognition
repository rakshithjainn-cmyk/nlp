
## requirements.txt
numpy
pandas
opencv-python
tensorflow>=2.10
keras
scikit-learn
matplotlib
streamlit
googletrans==3.1.0a0
tqdm
joblib
## .gitignore
__pycache__/
*.pyc
models/
logs/
data/
.env
## README.md
Place labeled image sequences under `data/` in per-class folders or use a CSV mapping `filepath,label` for training.

Train: python src/train.py --data_csv data/labels.csv
Run Streamlit UI: streamlit run app/streamlit_app.py
## src/data\_loader.py

```python
# src/data_loader.py
import os
import cv2
import numpy as np
import pandas as pd
from tensorflow.keras.utils import to_categorical


def load_image(path, target_size=(64,64)):
    img = cv2.imread(path)
    if img is None:
        raise FileNotFoundError(f"Image not found: {path}")
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, target_size)
    img = img.astype('float32')/255.0
    return img


def load_sequence(sequence_paths, target_size=(64,64), max_len=30):
    seq = []
    for p in sequence_paths[:max_len]:
        seq.append(load_image(p, target_size))
    # pad if shorter
    while len(seq) < max_len:
        seq.append(np.zeros((target_size[0], target_size[1], 3), dtype='float32'))
    return np.stack(seq)


def prepare_dataset_from_csv(csv_path, images_root='', max_len=30, target_size=(64,64)):
    """
    CSV format: sequence_id,frame_paths(comma-separated),label
    Or: filepath,label for single image classification dataset
    Return: X (num_samples, timesteps, h, w, c), y (num_samples, )
    """
    df = pd.read_csv(csv_path)
    X = []
    y = []
    for _, row in df.iterrows():
        if 'frame_paths' in row and pd.notna(row['frame_paths']):
            frames = row['frame_paths'].split(';')
            frames = [os.path.join(images_root, f.strip()) if images_root else f.strip() for f in frames]
            seq = load_sequence(frames, target_size=target_size, max_len=max_len)
            X.append(seq)
            y.append(row['label'])
        elif 'filepath' in row and pd.notna(row['filepath']):
            p = os.path.join(images_root, row['filepath']) if images_root else row['filepath']
            img = load_image(p, target_size=target_size)
            # convert to sequence length 1 + padding
            seq = np.concatenate([np.expand_dims(img,0), np.zeros((max_len-1, target_size[0], target_size[1],3))], axis=0)
            X.append(seq)
            y.append(row['label'])
        else:
            raise ValueError('CSV must contain either frame_paths or filepath columns')
    X = np.array(X)
    labels, uniques = pd.factorize(y)
    y = to_categorical(labels)
    return X, y, list(uniques)
```

---

## src/model.py

```python
# src/model.py
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, TimeDistributed, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LSTM, BatchNormalization
from tensorflow.keras.optimizers import Adam


def build_cnn_frame(input_shape=(64,64,3)):
    from tensorflow.keras import layers
    inp = Input(shape=input_shape)
    x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(inp)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2,2))(x)

    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2,2))(x)

    x = layers.Conv2D(128, (3,3), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2,2))(x)

    x = layers.Flatten()(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(0.4)(x)
    model = Model(inp, x, name='cnn_frame')
    return model


def build_cnn_lstm_model(timesteps=30, frame_shape=(64,64,3), num_classes=10):
    frame_model = build_cnn_frame(input_shape=frame_shape)
    seq_in = Input(shape=(timesteps, frame_shape[0], frame_shape[1], frame_shape[2]))
    x = TimeDistributed(frame_model)(seq_in)
    x = LSTM(256, return_sequences=False)(x)
    x = Dropout(0.4)(x)
    x = Dense(128, activation='relu')(x)
    out = Dense(num_classes, activation='softmax')(x)
    model = Model(seq_in, out)
    model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])
    return model
```

---

## src/train.py

```python
# src/train.py
import os
import argparse
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from src.data_loader import prepare_dataset_from_csv
from src.model import build_cnn_lstm_model
from sklearn.model_selection import train_test_split
import numpy as np


def main(args):
    X, y, label_map = prepare_dataset_from_csv(args.data_csv, images_root=args.images_root, max_len=args.max_len, target_size=(args.h,args.w))
    num_classes = y.shape[1]
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42, stratify=np.argmax(y,axis=1))

    model = build_cnn_lstm_model(timesteps=args.max_len, frame_shape=(args.h,args.w,3), num_classes=num_classes)
    os.makedirs('models', exist_ok=True)
    ckpt = ModelCheckpoint('models/best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
    rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)
    es = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)

    model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=args.epochs,
        batch_size=args.batch_size,
        callbacks=[ckpt, rlr, es]
    )
    # save label map
    import json
    with open('models/label_map.json', 'w') as f:
        json.dump(label_map, f)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_csv', type=str, required=True)
    parser.add_argument('--images_root', type=str, default='')
    parser.add_argument('--max_len', type=int, default=30)
    parser.add_argument('--h', type=int, default=64)
    parser.add_argument('--w', type=int, default=64)
    parser.add_argument('--epochs', type=int, default=30)
    parser.add_argument('--batch_size', type=int, default=8)
    args = parser.parse_args()
    main(args)
```

---

## src/predict.py

```python
# src/predict.py
import json
import numpy as np
from tensorflow.keras.models import load_model
from src.data_loader import load_sequence


def load_label_map(path='models/label_map.json'):
    with open(path,'r') as f:
        labels = json.load(f)
    return labels


def predict_sequence(frame_paths, model_path='models/best_model.h5', max_len=30, target_size=(64,64)):
    seq = load_sequence(frame_paths, target_size=target_size, max_len=max_len)
    seq = np.expand_dims(seq, axis=0)
    model = load_model(model_path)
    preds = model.predict(seq)
    idx = int(np.argmax(preds, axis=1)[0])
    return idx, preds[0]
```

---

## app/streamlit\_app.py

```python
# app/streamlit_app.py
import streamlit as st
from googletrans import Translator
import json
from src.predict import predict_sequence
import os

st.set_page_config(page_title='ISL Recognizer', layout='wide')

st.title('Indian Sign Language Recognition â€” Multilingual Translation')

with st.sidebar:
    st.header('Settings')
    model_path = st.text_input('Model path', value='models/best_model.h5')
    label_map_path = st.text_input('Label map path', value='models/label_map.json')
    target_lang = st.selectbox('Translate to', options=['en','hi','es','fr','de','bn','ta','te','ml','mr'])

st.write('Upload sequence frames (as multiple image files) or a zip of frames in correct order.')
uploaded = st.file_uploader('Upload frames', type=['png','jpg','jpeg','zip'], accept_multiple_files=True)

if uploaded:
    # if zip provided, extract
    img_paths = []
    tmp_dir = 'tmp_upload'
    os.makedirs(tmp_dir, exist_ok=True)
    for i, f in enumerate(uploaded):
        # save file
        outp = os.path.join(tmp_dir, f'{i}_{f.name}')
        with open(outp,'wb') as wf:
            wf.write(f.getbuffer())
        img_paths.append(outp)

    st.image([p for p in img_paths[:10]], width=120, caption=[os.path.basename(p) for p in img_paths[:10]])

    if st.button('Predict & Translate'):
        # load label map
        with open(label_map_path,'r') as f:
            label_map = json.load(f)
        idx, probs = predict_sequence(img_paths, model_path=model_path)
        label = label_map[idx]
        st.success(f'Predicted sign: {label} (class {idx})')
        translator = Translator()
        trans = translator.translate(label, dest=target_lang)
        st.info(f'Translation ({target_lang}): {trans.text}')

        st.write('Class probabilities:')
        for i,p in enumerate(probs):
            st.write(f"{i}: {label_map[i]} -> {p:.3f}")
```

---

## scripts/run\_train.sh

```bash
#!/usr/bin/env bash
python src/train.py --data_csv data/labels.csv --images_root data/frames --max_len 30 --epochs 20 --batch_size 8
```
